#file  -- ensemble.py --
import tensorflow as tf
import numpy as np
from tensorflow import keras
import matplotlib
matplotlib.use('GTK3Cairo')
from matplotlib import pyplot as plt


# "The data sets are  normalized  so  that  the  input  features  and  the
#    targets have zero mean and unit variance in the training set"
def normalize(x_train, y_train, x_test, y_test):

    x_train_mean = np.mean(x_train)
    x_train_n = x_train - x_train_mean
    x_train_std = np.std(x_train)
    x_train_n /= x_train_std
    # Currently I am not normalizing targets

    # y_train_mean = np.mean(y_train)
    # y_train -= y_train_mean
    # y_train_std = np.std(y_train)
    # y_train /= y_train_std
    if x_test is not None:
        x_test_n = x_test - x_train_mean
        x_test_n /= x_train_std
    # y_test -= y_train_mean
    # y_test /= y_train_std
    return (x_train_n, y_train), (x_test_n, y_test)

def load_data(seed, test_split, name):
    if name == 'toy':
        # y = x^3 + N(0,3^2)
        #toy data set generated by sampling 20 inputs x uniformly  at  random  in
        #the  interval [−4, 4]. For  each value  of x obtained,  the  corresponding
        #target y is  computed  as y = x^3 + N(0,3^2), We  fitted a neural network
        #with one layer and 100 hidden units to these data.
        np.random.seed(seed)
        x_train = np.random.uniform(-4.0,4.0,20)
        y_train = np.power(x_train,3) + np.random.normal(0, 3,20)

        x_test = None
        y_test = None

    if name == 'boston':
        boston_housing = keras.datasets.boston_housing
        (x_train, y_train), (x_test, y_test) = boston_housing.load_data(seed=seed,test_split=test_split)

    if name == 'mnist':
        mnist = keras.datasets.mnist
        (x_train, y_train),(x_test, y_test) = mnist.load_data()
        x_train, x_test = x_train / 255.0, x_test / 255.0

    #x_train, y_train, x_test, y_test = normalize(x_train, y_train, x_test, y_test)

    return (x_train, y_train), (x_test, y_test)

def get_network_shape(x_train, name):
    if name == 'toy':
        network_shape = (1,100,2)
    elif name == 'boston':
        network_shape = (x_train.shape[1],50,2)
    elif name == 'mnist':
        network_shape = (784, 200, 200, 200, 10)
    else:
        assert(False)

    return network_shape


def toy_plots(x_true, y_true, x_true_n, x_train, y_train, y_pred_teacher, y_pred_student):
    y_pred_teacher_mean = y_pred_teacher[:,0]
    y_pred_teacher_std3 = 3.0 * np.sqrt(y_pred_teacher[:,1])

    # Plot results
    plt.plot(x_true, y_pred_teacher_mean, '#000000')
    plt.fill_between(x_true, y_pred_teacher_mean-y_pred_teacher_std3,
                             y_pred_teacher_mean+y_pred_teacher_std3,
                     alpha=0.5, edgecolor='#4d4d4d', facecolor='#BEBEBE')

    plt.figure(1)
    plt.plot(x_true,y_true)
    plt.plot(x_train, y_train, 'bo')
    plt.title('Toy example - Teacher')
    plt.ylabel('y')
    plt.xlabel('x')
    plt.legend(['ensemble', 'y=x^3'], loc='upper left')

    # --------- distilled ------------
    y_pred_student_mean = y_pred_student[:,0]
    y_pred_student_std3 = 3.0 * np.sqrt(y_pred_student[:,1])
    plt.figure(2)
    # Plot results
    plt.plot(x_true, y_pred_student_mean, '#000000')
    plt.fill_between(x_true, y_pred_student_mean-y_pred_student_std3,
                             y_pred_student_mean+y_pred_student_std3,
                     alpha=0.5, edgecolor='#4d4d4d', facecolor='#BEBEBE')
    #plt.plot(x_true, y_pred_student[:,0])

    plt.plot(x_true,y_true)
    plt.plot(x_train, y_train, 'bo')
    plt.title('Toy example - Student')
    plt.ylabel('y')
    plt.xlabel('x')
    plt.legend(['student', 'y=x^3'], loc='upper left')
    plt.show()

def classification_plots(num_nets, max_nets, err_history, nll_history, teacher_history):

    if(len(err_history['student'])==0):
        plt.figure(1)
        plt.subplot(121)
        plt.xlabel('Number of nets')
        plt.title('Classification Error for Teacher')
        red, = plt.plot(num_nets, err_history['teacher'], 'r')
        plt.xticks(np.arange(0, max_nets+1, 5))
        plt.yticks(np.arange(1.0, 2.4, 0.2))

        plt.subplot(122)
        plt.xlabel('Number of nets')
        plt.title('NLL for teacher')
        red, = plt.plot(num_nets, nll_history['teacher'], 'r')
        plt.xticks(np.arange(0, max_nets+1, 5))
        plt.yticks(np.arange(0.02, 0.16, 0.02))

        #plt.subplot(133)
        #plt.xlabel('Number of nets')
        #plt.title('Brier Score')
        #red, = plt.plot(num_nets, ensemble_brier, 'r')
        #blue, = plt.plot(num_nets, distilled_brier, 'b')
        #plt.legend([red,blue], ['Ensemble', 'Distilled'])
        #plt.xticks(np.arange(0, max_nets+1, 5))
        #plt.yticks(np.arange(0.0014, 0.0034, 0.0002))

        plt.figure(2)
        plt.plot(teacher_history.history['loss'])
        plt.plot(teacher_history.history['val_loss'])
            #labels.append('loss ' + str(batch_size))
            #labels.append('loss_val ' + str(batch_size))

        plt.title('teacher loss')
        plt.ylabel('loss')
        plt.xlabel('epoch')
        plt.legend(['train', 'validation'], loc='upper left')

        #plt.legend(labels)
        #plt.show()

        plt.show()
    else:
        plt.figure(1)
        plt.subplot(121)
        plt.xlabel('Number of nets')
        plt.title('Classification Error')
        red, = plt.plot(num_nets, err_history['teacher'], 'r')
        blue, = plt.plot(num_nets, err_history['student'], 'b')
        plt.legend([red,blue], ['Teacher', 'Student'])
        plt.xticks(np.arange(0, max_nets+1, 5))
        plt.yticks(np.arange(1.0, 2.4, 0.2))

        plt.subplot(122)
        plt.xlabel('Number of nets')
        plt.title('NLL')
        red, = plt.plot(num_nets, nll_history['teacher'], 'r')
        blue, = plt.plot(num_nets, nll_history['student'], 'b')
        plt.legend([red,blue], ['Teacher', 'Student'])
        plt.xticks(np.arange(0, max_nets+1, 5))
        plt.yticks(np.arange(0.02, 0.16, 0.02))

        #plt.subplot(133)
        #plt.xlabel('Number of nets')
        #plt.title('Brier Score')
        #red, = plt.plot(num_nets, ensemble_brier, 'r')
        #blue, = plt.plot(num_nets, distilled_brier, 'b')
        #plt.legend([red,blue], ['Ensemble', 'Distilled'])
        #plt.xticks(np.arange(0, max_nets+1, 5))
        #plt.yticks(np.arange(0.0014, 0.0034, 0.0002))

        plt.figure(2)
        plt.plot(teacher_history.history['loss'])
        plt.plot(teacher_history.history['val_loss'])
            #labels.append('loss ' + str(batch_size))
            #labels.append('loss_val ' + str(batch_size))

        plt.title('teacher loss')
        plt.ylabel('loss')
        plt.xlabel('epoch')
        plt.legend(['train', 'validation'], loc='upper left')

        #plt.legend(labels)
        #plt.show()

        plt.show()

def create_classification_model(network_shape):
    return keras.models.Sequential([
           keras.layers.Flatten(input_shape=(28,28,)),
           keras.layers.Dense(200, activation=tf.nn.relu),
           keras.layers.BatchNormalization(),
           keras.layers.Dense(200, activation=tf.nn.relu),
           keras.layers.BatchNormalization(),
           keras.layers.Dense(200, activation=tf.nn.relu),
           keras.layers.BatchNormalization(),
           keras.layers.Dense(10)
           ])

"""
"We use the identical network architecture: 1-hidden layer NN with ReLU
nonlinearity [45], containing 50 hidden units for smaller datasets"

"We enforce the positivity constraint on the variance by passing the second
output through the softplus function log(1 + exp(·)), and add a minimum
variance (e.g. 1e-6) for numerical stability."
"""
def create_regression_model(network_shape):
    (input_dim, hidden_layer_units, output_dim) = network_shape

    model = keras.models.Sequential()
    model.add( keras.layers.Dense(hidden_layer_units, input_dim=input_dim,
                                    activation=tf.nn.relu) )

    logits = model.layers[-1].output
    mean_layer = keras.layers.Dense(1)(logits)
    variance_layer = keras.layers.Dense(1)(logits)

    variance_layer = keras.layers.Lambda(lambda x:
    keras.activations.softplus(x) + 1E-6)(variance_layer)

    output = keras.layers.concatenate([mean_layer, variance_layer])
    return keras.Model(model.input, output)
